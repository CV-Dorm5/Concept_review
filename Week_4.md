- 잘한 점 : 논문 발표를 위해 논문을 읽고 이야기하는 시간을 가짐, 피어세션시간에 어려운 부분에대해 토의를하고 알아가는 시간이 늘어남,
밝고, 친근한 분위기

- 못한 점 : 다른 활동으로 인해 현재 강의에 충실하지 못함
    
    
- 도전할 것 : 논문 발표시간을 더 가지는 것에대해 논의

- 키워드 : 5조 왜 유명한가

- 이번주 내가 했던 행동, 작업, 계획, 일정, 소통, 활동에 대한 회고

---

정태민: 깃 내용 반드시 정리해서 내것으로 체화할 것! 새로 읽은 convNext 복습

손인수 : resnet 등 computer vision 기초에 대해서 배울 수 있어서 좋았고, 피어세션은 이번주도 편했습니다. 전반적으로 만족스러운 학습환경입니다.

고은성 : 논문처음 읽고 본격적인 공부를 해보려니 내가 얼마나 부족한지 깨달았습니다. 죽었다 생각하고 이제 공부만 하겠습니다.

이유민 :  ResNet 을 직접 구현해보는 과정에서 논문을 이해하는 데 더 도움을 받을 수 있었다. 멘토링 활동을 cv 9조와 논문 리뷰를 했었는데 ,상호간의 질문을 통해 추가적인 공부를 할 수 있는 기회가 되었고 논문 읽는 것에 거부감을 줄일 수 있었다.

안종진 : 논문 읽기의 중요성을 알게되었습니다. 기초 부분을 돌아가 복습했습니다. 스페셜 피어세션을하면서 이전보다는 조금 더 활발하게 소통했습니다.

최현영 : ResNet 구현을 numpy로만 했던 적도 있는데 구조적 특성에대해 오래 안봤다고 skip connection말고 나머지는 거의 모르는게 좀 충격적이었다. 공부를 계속해야하고 구현실력이 녹슬지 않도록 노력해야한다고 생각한다

## 월

---

- 인수 : 강의 3강, 면접 준비, 면접 후 스케쥴에 맞춰서 생활이 가능할 것 같다.
- 종진 : 논문 리뷰 준비, 처음하는 논문 읽기라 어려움을 느꼈다, 강의 1강
- 태민 : 강의 2강 과제 1 ResNet 구현, Convolution Block에서 Downsampling이 일어나야되는데 과제의 코드로는 일어나지 않아서 문제 해결 중
- 현영 : 강의 3강, 논문 리뷰 준비, 정리
- 유민 : 강의 1강, 눈문 리뷰 준비, 이전에 본 논문과 달리 ViT와 비교하며 진행되는 논문이라 난해했다.
- 은성 : 강의 1강, 논문 리뷰 준비, 첫 논문 읽기라 어려웠다.
- 피어세션 내용 공유
    - Inductive bias란? 사람이 데이터를 해석하는데 가진 편견을 모델에 넣어주는 것?
    - CNN의 sliding window 그 자체?, 이미지 데이터는 인접할 수록 연관성이 크다는 점
    - Transpomer에서 positional embedding은 inductive bias인가?
    - RNN에서 데이터의 시간과 연속된 순서
    

### 화

---

- 태민 : Git 특강, 논문 발표 준비
- 유민 : Git 특강, 논문 발표 준비
- 인수 : Git 특강, 면접 준비
- 현영 : Git 특강, 논문 발표 준비
- 종진 : Git 특강, 논문 발표 준비
- 은성 : Git 특강, 논문 발표 준비
- 피어세션 내용 공유
    - I**teration (이터레이션)**:
        - 이터레이션은 학습 과정에서 단일 배치가 네트워크를 통과하고, 가중치가 업데이트되는 한 번의 과정을 의미합니다.
        - 예를 들어, 1000개의 트레이닝 샘플이 있고 배치 크기가 100이라면, 10개의 이터레이션을 통해 모든 샘플이 네트워크를 한 번씩 통과합니다.
    - **Epoch (에포크)**:
        - 에포크는 전체 트레이닝 데이터셋이 네트워크를 한 번 통과하는 것을 의미합니다.
        - 한 에포크는 여러 이터레이션으로 구성됩니다. 예를 들어, 전체 데이터셋이 모든 이터레이션을 거쳐 한 번씩 네트워크를 통과하면 한 에포크가 완료됩니다.
        - 에포크의 수는 모델이 트레이닝 데이터를 얼마나 많이 보게 할지 결정합니다. 너무 적으면 학습이 부족할 수 있고, 너무 많으면 오버피팅의 위험이 있습니다.
    - **Batch (배치)**:
        - 배치는 네트워크를 통과하는 데이터의 한 그룹입니다.
        - 전체 트레이닝 데이터셋을 작은 배치로 나누어서 처리합니다. 이는 메모리 효율성을 높이고, 학습 속도를 개선하는 데 도움이 됩니다.
        - 배치 크기는 모델 학습에 중요한 하이퍼파라미터로, 너무 크면 메모리 문제가 발생할 수 있고, 너무 작으면 학습이 불안정해질 수 있습니다.

### 수

---

- 유민 : 강의 5강, 과제 1 수행중, 논문 발표 자료 준비
- 종진 : 강의 3강, 과제 1 수행중, 논문 발표 자료 준비
- 현영 : 강의 3강, 과제 1 수행중, 논문 발표 자료 준비
- 인수 : 강의 5강, 과제 1 수행중, 면접 준비
- 태민 : 강의 4강 과제 1,2 완료, 논문 발표 자료 준비
- 은성 : 강의 2강, 과제 1 수행중, 논문 발표 자료 준비
- 피어세션 내용 공유
    - ViT
    Segment Anything Model
    Separate downsampling
    - Layer normalization
    - Batch normalization

멘토님과 CV 9조와 논문 리뷰

### 목

---

- 은성 : 4강 수강중, 과제 수행 예정
- 유민 : 과제 2까지 완료, 과제 3 수행중 마무리 예정, data visualization 2개 수강예정
- 인수 : 면접
- 종진 : 과제 1 수행중 과제 3 목표, data visualization 수강
- 현영 : 과제 3 수행중, data visualization 수강 완료
- 태민 : 과제 3 완료 후 면접 준비

### 금

---

- 유민 : 논문 발표 질문 조사 / 피어세션을 통해 9조와 논문 발표 답변 공유
- 태민 : 논문 발표 질문 조사 / 피어세션을 통해 9조와 논문 발표 답변 공유
- 현영 : 논문 발표 질문 조사 / 피어세션을 통해 9조와 논문 발표 답변 공유
- 인수 : 논문 발표 질문 조사 / 피어세션을 통해 9조와 논문 발표 답변 공유
- 종진 : 논문 발표 질문 조사 / 피어세션을 통해 9조와 논문 발표 답변 공유
- 은성 : 논문 발표 질문 조사 / 피어세션을 통해 9조와 논문 발표 답변 공유
